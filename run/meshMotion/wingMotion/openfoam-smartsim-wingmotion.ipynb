{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee2dbd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmartRedis Library@15-21-39:WARNING: Environment variable SR_LOG_FILE is not set. Defaulting to stdout\n",
      "SmartRedis Library@15-21-39:WARNING: Environment variable SR_LOG_LEVEL is not set. Defaulting to INFO\n",
      "15:29:55 mma120347 SmartSim[468047] INFO of_model(468438): Completed\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "from PyFoam.RunDictionary.ParsedParameterFile import ParsedParameterFile\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from smartsim import Experiment\n",
    "from smartredis import Client\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams[\"figure.dpi\"] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# For calling pre-processing scripts\n",
    "import subprocess\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_layers, layer_width, input_size, output_size, activation_fn):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, layer_width))\n",
    "        layers.append(activation_fn)\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(layer_width, layer_width))\n",
    "            layers.append(activation_fn)\n",
    "\n",
    "        layers.append(nn.Linear(layer_width, output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "def sort_tensors_by_names(tensors, tensor_names):\n",
    "    # Pair each tensor with its name and sort by the name\n",
    "    pairs = sorted(zip(tensor_names, tensors))\n",
    "\n",
    "    # Extract the sorted tensors\n",
    "    tensor_names_sorted, tensors_sorted = zip(*pairs)\n",
    "\n",
    "    # Convert back to list if needed\n",
    "    tensor_names_sorted = list(tensor_names_sorted)\n",
    "    tensors_sorted = list(tensors_sorted)\n",
    "\n",
    "    return tensors_sorted, tensor_names_sorted\n",
    "\n",
    "exp = Experiment(\"mesh-motion\", launcher=\"local\")\n",
    "\n",
    "db = exp.create_database(port=8000,       # database port\n",
    "                         interface=\"lo\")  # network interface to use\n",
    "exp.start(db)\n",
    "\n",
    "# Connect the python client to the smartredis database\n",
    "client = Client(address=db.get_address()[0], cluster=False)\n",
    "\n",
    "num_mpi_ranks = 4\n",
    "\n",
    "of_rs = exp.create_run_settings(exe=\"pimpleFoam\", exe_args=\"-case wingMotion2D_pimpleFoam -parallel\", \n",
    "                                run_command=\"mpirun\", \n",
    "                                run_args={\"np\": f\"{num_mpi_ranks}\"})\n",
    "\n",
    "of_model = exp.create_model(name=\"of_model\", run_settings=of_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7838a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning case /home/MMAuser/Projects/research/openfoam/openfoam-smartsim/run/meshMotion/wingMotion/wingMotion_snappyHexMesh\n",
      "Cleaning case /home/MMAuser/Projects/research/openfoam/openfoam-smartsim/run/meshMotion/wingMotion/wingMotion2D_simpleFoam\n",
      "Cleaning case /home/MMAuser/Projects/research/openfoam/openfoam-smartsim/run/meshMotion/wingMotion/wingMotion2D_pimpleFoam\n",
      "Allclean executed with return code: 0\n",
      "Running blockMesh on /home/MMAuser/Projects/research/openfoam/openfoam-smartsim/run/meshMotion/wingMotion/wingMotion_snappyHexMesh\n",
      "Running snappyHexMesh on /home/MMAuser/Projects/research/openfoam/openfoam-smartsim/run/meshMotion/wingMotion/wingMotion_snappyHexMesh\n",
      "Running extrudeMesh on /home/MMAuser/Projects/research/openfoam/openfoam-smartsim/run/meshMotion/wingMotion/wingMotion2D_simpleFoam\n",
      "Running createPatch on /home/MMAuser/Projects/research/openfoam/openfoam-smartsim/run/meshMotion/wingMotion/wingMotion2D_simpleFoam\n",
      "Restore 0/ from 0.orig/\n",
      "Running simpleFoam on /home/MMAuser/Projects/research/openfoam/openfoam-smartsim/run/meshMotion/wingMotion/wingMotion2D_simpleFoam\n",
      "Restore 0/ from 0.orig/\n",
      "Running mapFields on /home/MMAuser/Projects/research/openfoam/openfoam-smartsim/run/meshMotion/wingMotion/wingMotion2D_pimpleFoam\n",
      "Running decomposePar on /home/MMAuser/Projects/research/openfoam/openfoam-smartsim/run/meshMotion/wingMotion/wingMotion2D_pimpleFoam\n",
      "Allrun.pre executed with return code: 0\n",
      "Time step 1\n",
      "Saving model MLP\n",
      "Time step 2\n",
      "Saving model MLP\n",
      "Time step 3\n",
      "Saving model MLP\n",
      "Time step 4\n",
      "Saving model MLP\n",
      "Time step 5\n",
      "Saving model MLP\n",
      "Time step 6\n",
      "Saving model MLP\n",
      "Time step 7\n",
      "Saving model MLP\n",
      "Time step 8\n",
      "Saving model MLP\n",
      "Time step 9\n",
      "Saving model MLP\n",
      "Time step 10\n",
      "Saving model MLP\n",
      "Time step 11\n",
      "Saving model MLP\n",
      "Time step 12\n",
      "Saving model MLP\n",
      "Time step 13\n",
      "Saving model MLP\n",
      "Time step 14\n",
      "Saving model MLP\n",
      "Time step 15\n",
      "Saving model MLP\n",
      "Time step 16\n",
      "Saving model MLP\n",
      "Time step 17\n",
      "Saving model MLP\n",
      "Time step 18\n",
      "Saving model MLP\n",
      "Time step 19\n",
      "Saving model MLP\n",
      "Time step 20\n",
      "Saving model MLP\n",
      "Time step 21\n",
      "Saving model MLP\n",
      "Time step 22\n",
      "Saving model MLP\n",
      "Time step 23\n",
      "Saving model MLP\n",
      "Time step 24\n",
      "Saving model MLP\n",
      "Time step 25\n",
      "Saving model MLP\n",
      "Time step 26\n",
      "Saving model MLP\n",
      "Time step 27\n",
      "Saving model MLP\n",
      "Time step 28\n",
      "Saving model MLP\n",
      "Time step 29\n",
      "Saving model MLP\n",
      "Time step 30\n",
      "Saving model MLP\n",
      "Time step 31\n",
      "Saving model MLP\n",
      "Time step 32\n",
      "Saving model MLP\n",
      "Time step 33\n",
      "Saving model MLP\n",
      "Time step 34\n",
      "Saving model MLP\n",
      "Time step 35\n",
      "Saving model MLP\n",
      "Time step 36\n",
      "Saving model MLP\n",
      "Time step 37\n",
      "Saving model MLP\n",
      "Time step 38\n",
      "Saving model MLP\n",
      "Time step 39\n",
      "Saving model MLP\n",
      "Time step 40\n",
      "Saving model MLP\n",
      "Time step 41\n",
      "Saving model MLP\n",
      "Time step 42\n",
      "Saving model MLP\n",
      "Time step 43\n",
      "Saving model MLP\n",
      "Time step 44\n",
      "Saving model MLP\n",
      "Time step 45\n",
      "Saving model MLP\n",
      "Time step 46\n",
      "Saving model MLP\n",
      "Time step 47\n",
      "Saving model MLP\n",
      "Time step 48\n",
      "Saving model MLP\n",
      "Time step 49\n",
      "Saving model MLP\n",
      "Time step 50\n",
      "Saving model MLP\n",
      "Time step 51\n",
      "Saving model MLP\n",
      "Time step 52\n",
      "Saving model MLP\n",
      "Time step 53\n",
      "Saving model MLP\n",
      "Time step 54\n",
      "Saving model MLP\n",
      "Time step 55\n",
      "Saving model MLP\n",
      "Time step 56\n",
      "Saving model MLP\n",
      "Time step 57\n",
      "Saving model MLP\n",
      "Time step 58\n",
      "Saving model MLP\n",
      "Time step 59\n",
      "Saving model MLP\n",
      "Time step 60\n",
      "Saving model MLP\n",
      "Time step 61\n",
      "Saving model MLP\n",
      "Time step 62\n",
      "Saving model MLP\n",
      "Time step 63\n",
      "Saving model MLP\n",
      "Time step 64\n",
      "Saving model MLP\n",
      "Time step 65\n",
      "Saving model MLP\n",
      "Time step 66\n",
      "Saving model MLP\n",
      "Time step 67\n",
      "Saving model MLP\n",
      "Time step 68\n",
      "Saving model MLP\n",
      "Time step 69\n",
      "Saving model MLP\n",
      "Time step 70\n",
      "Saving model MLP\n",
      "Time step 71\n",
      "Saving model MLP\n",
      "Time step 72\n",
      "Saving model MLP\n",
      "Time step 73\n",
      "Saving model MLP\n",
      "Time step 74\n",
      "Saving model MLP\n",
      "Time step 75\n",
      "Saving model MLP\n",
      "Time step 76\n",
      "Saving model MLP\n",
      "Time step 77\n",
      "Saving model MLP\n",
      "Time step 78\n",
      "Saving model MLP\n",
      "Time step 79\n",
      "Saving model MLP\n",
      "Time step 80\n",
      "Saving model MLP\n",
      "Time step 81\n",
      "Saving model MLP\n",
      "Time step 82\n",
      "Saving model MLP\n",
      "Time step 83\n",
      "Saving model MLP\n",
      "Time step 84\n",
      "Saving model MLP\n",
      "Time step 85\n",
      "Saving model MLP\n",
      "Time step 86\n",
      "Saving model MLP\n",
      "Time step 87\n",
      "Saving model MLP\n",
      "Time step 88\n",
      "Saving model MLP\n",
      "Time step 89\n",
      "Saving model MLP\n",
      "Time step 90\n",
      "Saving model MLP\n",
      "Time step 91\n",
      "Saving model MLP\n",
      "Time step 92\n",
      "Saving model MLP\n",
      "Time step 93\n",
      "Saving model MLP\n",
      "Time step 94\n",
      "Saving model MLP\n",
      "Time step 95\n",
      "Saving model MLP\n",
      "Time step 96\n",
      "Saving model MLP\n",
      "Time step 97\n",
      "Saving model MLP\n",
      "Time step 98\n",
      "Saving model MLP\n",
      "Time step 99\n",
      "Saving model MLP\n",
      "Time step 100\n",
      "Saving model MLP\n",
      "Time step 101\n",
      "Saving model MLP\n",
      "Time step 102\n",
      "Saving model MLP\n",
      "Time step 103\n",
      "Saving model MLP\n",
      "Time step 104\n",
      "Saving model MLP\n",
      "Time step 105\n",
      "Saving model MLP\n",
      "Time step 106\n",
      "Saving model MLP\n",
      "Time step 107\n",
      "Saving model MLP\n",
      "Time step 108\n",
      "Saving model MLP\n",
      "Time step 109\n",
      "Saving model MLP\n",
      "Time step 110\n",
      "Saving model MLP\n",
      "Time step 111\n",
      "Saving model MLP\n",
      "Time step 112\n",
      "Saving model MLP\n",
      "Time step 113\n",
      "Saving model MLP\n",
      "Time step 114\n",
      "Saving model MLP\n",
      "Time step 115\n",
      "Saving model MLP\n",
      "Time step 116\n",
      "Saving model MLP\n",
      "Time step 117\n",
      "Saving model MLP\n",
      "Time step 118\n",
      "Saving model MLP\n",
      "Time step 119\n",
      "Saving model MLP\n",
      "Time step 120\n",
      "Saving model MLP\n",
      "Time step 121\n",
      "Saving model MLP\n",
      "Time step 122\n",
      "Saving model MLP\n",
      "Time step 123\n",
      "Saving model MLP\n",
      "Time step 124\n",
      "Saving model MLP\n",
      "Time step 125\n",
      "Saving model MLP\n",
      "Time step 126\n",
      "Saving model MLP\n",
      "Time step 127\n",
      "Saving model MLP\n",
      "Time step 128\n",
      "Saving model MLP\n",
      "Time step 129\n",
      "Saving model MLP\n",
      "Time step 130\n",
      "Saving model MLP\n",
      "Time step 131\n",
      "Saving model MLP\n",
      "Time step 132\n",
      "Saving model MLP\n",
      "Time step 133\n",
      "Saving model MLP\n",
      "Time step 134\n",
      "Saving model MLP\n",
      "Time step 135\n",
      "Saving model MLP\n",
      "Time step 136\n",
      "Saving model MLP\n",
      "Time step 137\n",
      "Saving model MLP\n",
      "Time step 138\n",
      "Saving model MLP\n",
      "Time step 139\n",
      "Saving model MLP\n",
      "Time step 140\n",
      "Saving model MLP\n",
      "Time step 141\n",
      "Saving model MLP\n",
      "Time step 142\n",
      "Saving model MLP\n",
      "Time step 143\n",
      "Saving model MLP\n",
      "Time step 144\n",
      "Saving model MLP\n",
      "Time step 145\n",
      "Saving model MLP\n",
      "Time step 146\n",
      "Saving model MLP\n",
      "Time step 147\n",
      "Saving model MLP\n",
      "Time step 148\n",
      "Saving model MLP\n",
      "Time step 149\n",
      "Saving model MLP\n",
      "Time step 150\n",
      "Saving model MLP\n",
      "Time step 151\n",
      "Saving model MLP\n",
      "Time step 152\n",
      "Saving model MLP\n",
      "Time step 153\n",
      "Saving model MLP\n",
      "Time step 154\n",
      "Saving model MLP\n",
      "Time step 155\n",
      "Saving model MLP\n",
      "Time step 156\n",
      "Saving model MLP\n",
      "Time step 157\n",
      "Saving model MLP\n",
      "Time step 158\n",
      "Saving model MLP\n",
      "Time step 159\n",
      "Saving model MLP\n",
      "Time step 160\n",
      "Saving model MLP\n",
      "Time step 161\n",
      "Saving model MLP\n",
      "Time step 162\n",
      "Saving model MLP\n",
      "Time step 163\n",
      "Saving model MLP\n",
      "Time step 164\n",
      "Saving model MLP\n",
      "Time step 165\n",
      "Saving model MLP\n",
      "Time step 166\n",
      "Saving model MLP\n",
      "Time step 167\n",
      "Saving model MLP\n",
      "Time step 168\n",
      "Saving model MLP\n",
      "Time step 169\n",
      "Saving model MLP\n",
      "Time step 170\n",
      "Saving model MLP\n",
      "Time step 171\n",
      "Saving model MLP\n",
      "Time step 172\n",
      "Saving model MLP\n",
      "Time step 173\n",
      "Saving model MLP\n",
      "Time step 174\n",
      "Saving model MLP\n",
      "Time step 175\n",
      "Saving model MLP\n",
      "Time step 176\n",
      "Saving model MLP\n",
      "Time step 177\n",
      "Saving model MLP\n",
      "Time step 178\n",
      "Saving model MLP\n",
      "Time step 179\n",
      "Saving model MLP\n",
      "Time step 180\n",
      "Saving model MLP\n",
      "Time step 181\n",
      "Saving model MLP\n",
      "Time step 182\n",
      "Saving model MLP\n",
      "Time step 183\n",
      "Saving model MLP\n",
      "Time step 184\n",
      "Saving model MLP\n",
      "Time step 185\n",
      "Saving model MLP\n",
      "Time step 186\n",
      "Saving model MLP\n",
      "Time step 187\n",
      "Saving model MLP\n",
      "Time step 188\n",
      "Saving model MLP\n",
      "Time step 189\n",
      "Saving model MLP\n",
      "Time step 190\n",
      "Saving model MLP\n",
      "Time step 191\n",
      "Saving model MLP\n",
      "Time step 192\n",
      "Saving model MLP\n",
      "Time step 193\n",
      "Saving model MLP\n",
      "Time step 194\n",
      "Saving model MLP\n",
      "Time step 195\n",
      "Saving model MLP\n",
      "Time step 196\n",
      "Saving model MLP\n",
      "Time step 197\n",
      "Saving model MLP\n",
      "Time step 198\n",
      "Saving model MLP\n",
      "Time step 199\n",
      "Saving model MLP\n",
      "Time step 200\n",
      "Saving model MLP\n",
      "Time step 201\n",
      "Saving model MLP\n",
      "Time step 202\n",
      "Saving model MLP\n",
      "Time step 203\n",
      "Saving model MLP\n",
      "Time step 204\n",
      "Saving model MLP\n",
      "Time step 205\n",
      "Saving model MLP\n",
      "Time step 206\n",
      "Saving model MLP\n",
      "Time step 207\n",
      "Saving model MLP\n",
      "Time step 208\n",
      "Saving model MLP\n",
      "Time step 209\n",
      "Saving model MLP\n",
      "Time step 210\n",
      "Saving model MLP\n",
      "Time step 211\n",
      "Saving model MLP\n",
      "Time step 212\n",
      "Saving model MLP\n",
      "Time step 213\n",
      "Saving model MLP\n",
      "Time step 214\n",
      "Saving model MLP\n",
      "Time step 215\n",
      "Saving model MLP\n",
      "Time step 216\n",
      "Saving model MLP\n",
      "Time step 217\n",
      "Saving model MLP\n",
      "Time step 218\n",
      "Saving model MLP\n",
      "Time step 219\n",
      "Saving model MLP\n",
      "Time step 220\n",
      "Saving model MLP\n",
      "Time step 221\n",
      "Saving model MLP\n",
      "Time step 222\n",
      "Saving model MLP\n",
      "Time step 223\n",
      "Saving model MLP\n",
      "Time step 224\n",
      "Saving model MLP\n",
      "Time step 225\n",
      "Saving model MLP\n",
      "Time step 226\n",
      "Saving model MLP\n",
      "Time step 227\n",
      "Saving model MLP\n",
      "Time step 228\n",
      "Saving model MLP\n",
      "Time step 229\n",
      "Saving model MLP\n",
      "Time step 230\n",
      "Saving model MLP\n",
      "Time step 231\n",
      "Saving model MLP\n",
      "Time step 232\n",
      "Saving model MLP\n",
      "Time step 233\n",
      "Saving model MLP\n",
      "Time step 234\n",
      "Saving model MLP\n",
      "Time step 235\n",
      "Saving model MLP\n",
      "Time step 236\n",
      "Saving model MLP\n",
      "Time step 237\n",
      "Saving model MLP\n",
      "Time step 238\n",
      "Saving model MLP\n",
      "Time step 239\n",
      "Saving model MLP\n",
      "Time step 240\n",
      "Saving model MLP\n",
      "Time step 241\n",
      "Saving model MLP\n",
      "Time step 242\n",
      "Saving model MLP\n",
      "Time step 243\n",
      "Saving model MLP\n",
      "Time step 244\n",
      "Saving model MLP\n",
      "Time step 245\n",
      "Saving model MLP\n",
      "Time step 246\n",
      "Saving model MLP\n",
      "Time step 247\n",
      "Saving model MLP\n",
      "Time step 248\n",
      "Saving model MLP\n",
      "Time step 249\n",
      "Saving model MLP\n",
      "Time step 250\n",
      "Saving model MLP\n",
      "Time step 251\n",
      "Saving model MLP\n",
      "Time step 252\n",
      "Saving model MLP\n",
      "Time step 253\n",
      "Saving model MLP\n",
      "Time step 254\n",
      "Saving model MLP\n",
      "Time step 255\n",
      "Saving model MLP\n",
      "Time step 256\n",
      "Saving model MLP\n",
      "Time step 257\n",
      "Saving model MLP\n",
      "Time step 258\n",
      "Saving model MLP\n",
      "Time step 259\n",
      "Saving model MLP\n",
      "Time step 260\n",
      "Saving model MLP\n",
      "Time step 261\n",
      "Saving model MLP\n",
      "Time step 262\n",
      "Saving model MLP\n",
      "Time step 263\n",
      "Saving model MLP\n",
      "Time step 264\n",
      "Saving model MLP\n",
      "Time step 265\n",
      "Saving model MLP\n",
      "Time step 266\n",
      "Saving model MLP\n",
      "Time step 267\n",
      "Saving model MLP\n",
      "Time step 268\n",
      "Saving model MLP\n",
      "Time step 269\n",
      "Saving model MLP\n",
      "Time step 270\n",
      "Saving model MLP\n",
      "Time step 271\n",
      "Saving model MLP\n",
      "Time step 272\n",
      "Saving model MLP\n",
      "Time step 273\n",
      "Saving model MLP\n",
      "Time step 274\n",
      "Saving model MLP\n",
      "Time step 275\n",
      "Saving model MLP\n",
      "Time step 276\n",
      "Saving model MLP\n",
      "Time step 277\n",
      "Saving model MLP\n",
      "Time step 278\n",
      "Saving model MLP\n",
      "Time step 279\n",
      "Saving model MLP\n",
      "Time step 280\n",
      "Saving model MLP\n",
      "Time step 281\n",
      "Saving model MLP\n",
      "Time step 282\n",
      "Saving model MLP\n",
      "Time step 283\n",
      "Saving model MLP\n",
      "Time step 284\n",
      "Saving model MLP\n",
      "Time step 285\n",
      "Saving model MLP\n",
      "Time step 286\n",
      "Saving model MLP\n",
      "Time step 287\n",
      "Saving model MLP\n",
      "Time step 288\n",
      "Saving model MLP\n",
      "Time step 289\n",
      "Saving model MLP\n",
      "Time step 290\n",
      "Saving model MLP\n",
      "Time step 291\n",
      "Saving model MLP\n",
      "Time step 292\n",
      "Saving model MLP\n",
      "Time step 293\n",
      "Saving model MLP\n",
      "Time step 294\n",
      "Saving model MLP\n",
      "Time step 295\n",
      "Saving model MLP\n",
      "Time step 296\n",
      "Saving model MLP\n",
      "Time step 297\n",
      "Saving model MLP\n",
      "Time step 298\n",
      "Saving model MLP\n",
      "Time step 299\n",
      "Saving model MLP\n",
      "Time step 300\n",
      "Saving model MLP\n",
      "Time step 301\n",
      "Saving model MLP\n",
      "Time step 302\n",
      "Saving model MLP\n",
      "Time step 303\n",
      "Saving model MLP\n",
      "Time step 304\n",
      "Saving model MLP\n",
      "Time step 305\n",
      "Saving model MLP\n",
      "Time step 306\n",
      "Saving model MLP\n",
      "Time step 307\n",
      "Saving model MLP\n",
      "Time step 308\n",
      "Saving model MLP\n",
      "Time step 309\n",
      "Saving model MLP\n",
      "Time step 310\n",
      "Saving model MLP\n",
      "Time step 311\n",
      "Saving model MLP\n",
      "Time step 312\n",
      "Saving model MLP\n",
      "Time step 313\n",
      "Saving model MLP\n",
      "Time step 314\n",
      "Saving model MLP\n",
      "Time step 315\n",
      "Saving model MLP\n",
      "Time step 316\n",
      "Saving model MLP\n",
      "Time step 317\n",
      "Saving model MLP\n",
      "Time step 318\n",
      "Saving model MLP\n",
      "Time step 319\n",
      "Saving model MLP\n",
      "Time step 320\n",
      "Saving model MLP\n",
      "Time step 321\n",
      "Saving model MLP\n",
      "Time step 322\n",
      "Saving model MLP\n",
      "Time step 323\n",
      "Saving model MLP\n",
      "Time step 324\n",
      "Saving model MLP\n",
      "Time step 325\n",
      "Saving model MLP\n",
      "Time step 326\n",
      "Saving model MLP\n",
      "Time step 327\n",
      "Saving model MLP\n",
      "Time step 328\n",
      "Saving model MLP\n",
      "Time step 329\n",
      "Saving model MLP\n",
      "Time step 330\n",
      "Saving model MLP\n",
      "Time step 331\n",
      "Saving model MLP\n",
      "Time step 332\n",
      "Saving model MLP\n",
      "Time step 333\n",
      "Saving model MLP\n",
      "Time step 334\n",
      "Saving model MLP\n",
      "Time step 335\n",
      "Saving model MLP\n",
      "Time step 336\n",
      "Saving model MLP\n",
      "Time step 337\n",
      "Saving model MLP\n",
      "Time step 338\n",
      "Saving model MLP\n",
      "Time step 339\n",
      "Saving model MLP\n",
      "Time step 340\n",
      "Saving model MLP\n",
      "Time step 341\n",
      "Saving model MLP\n",
      "Time step 342\n",
      "Saving model MLP\n",
      "Time step 343\n",
      "Saving model MLP\n",
      "Time step 344\n",
      "Saving model MLP\n",
      "Time step 345\n",
      "Saving model MLP\n",
      "Time step 346\n",
      "Saving model MLP\n",
      "Time step 347\n",
      "Saving model MLP\n",
      "Time step 348\n",
      "Saving model MLP\n",
      "Time step 349\n",
      "Saving model MLP\n",
      "Time step 350\n",
      "Saving model MLP\n",
      "Time step 351\n",
      "Saving model MLP\n",
      "Time step 352\n",
      "Saving model MLP\n",
      "Time step 353\n",
      "Saving model MLP\n",
      "Time step 354\n",
      "Saving model MLP\n",
      "Time step 355\n",
      "Saving model MLP\n",
      "Time step 356\n",
      "Saving model MLP\n",
      "Time step 357\n",
      "Saving model MLP\n",
      "Time step 358\n",
      "Saving model MLP\n",
      "Time step 359\n",
      "Saving model MLP\n",
      "Time step 360\n",
      "Saving model MLP\n",
      "Time step 361\n",
      "Saving model MLP\n",
      "Time step 362\n",
      "Saving model MLP\n",
      "Time step 363\n",
      "Saving model MLP\n",
      "Time step 364\n",
      "Saving model MLP\n",
      "Time step 365\n",
      "Saving model MLP\n",
      "Time step 366\n",
      "Saving model MLP\n",
      "Time step 367\n",
      "Saving model MLP\n",
      "Time step 368\n",
      "Saving model MLP\n",
      "Time step 369\n",
      "Saving model MLP\n",
      "Time step 370\n",
      "Saving model MLP\n",
      "Time step 371\n",
      "Saving model MLP\n",
      "Time step 372\n",
      "Saving model MLP\n",
      "Time step 373\n",
      "Saving model MLP\n",
      "Time step 374\n",
      "Saving model MLP\n",
      "Time step 375\n",
      "Saving model MLP\n",
      "Time step 376\n",
      "Saving model MLP\n",
      "Time step 377\n",
      "Saving model MLP\n",
      "Time step 378\n",
      "Saving model MLP\n",
      "Time step 379\n",
      "Saving model MLP\n",
      "Time step 380\n",
      "Saving model MLP\n",
      "Time step 381\n",
      "Saving model MLP\n",
      "Time step 382\n",
      "Saving model MLP\n",
      "Time step 383\n",
      "Saving model MLP\n",
      "Time step 384\n",
      "Saving model MLP\n",
      "Time step 385\n",
      "Saving model MLP\n",
      "Time step 386\n",
      "Saving model MLP\n",
      "Time step 387\n",
      "Saving model MLP\n",
      "Time step 388\n",
      "Saving model MLP\n",
      "Time step 389\n",
      "Saving model MLP\n",
      "Time step 390\n",
      "Saving model MLP\n",
      "Time step 391\n",
      "Saving model MLP\n",
      "Time step 392\n",
      "Saving model MLP\n",
      "Time step 393\n",
      "Saving model MLP\n",
      "Time step 394\n",
      "Saving model MLP\n",
      "Time step 395\n",
      "Saving model MLP\n",
      "Time step 396\n",
      "Saving model MLP\n",
      "Time step 397\n",
      "Saving model MLP\n",
      "Time step 398\n",
      "Saving model MLP\n",
      "Time step 399\n",
      "Saving model MLP\n",
      "Time step 400\n",
      "Saving model MLP\n",
      "Time step 401\n",
      "Saving model MLP\n",
      "Time step 402\n",
      "Saving model MLP\n",
      "Time step 403\n",
      "Saving model MLP\n",
      "Time step 404\n",
      "Saving model MLP\n",
      "Time step 405\n",
      "Saving model MLP\n",
      "Time step 406\n",
      "Saving model MLP\n",
      "Time step 407\n",
      "Saving model MLP\n",
      "Time step 408\n",
      "Saving model MLP\n",
      "Time step 409\n",
      "Saving model MLP\n",
      "Time step 410\n",
      "Saving model MLP\n",
      "Time step 411\n",
      "Saving model MLP\n",
      "Time step 412\n",
      "Saving model MLP\n",
      "Time step 413\n",
      "Saving model MLP\n",
      "Time step 414\n",
      "Saving model MLP\n",
      "Time step 415\n",
      "Saving model MLP\n",
      "Time step 416\n",
      "Saving model MLP\n",
      "Time step 417\n",
      "Saving model MLP\n",
      "Time step 418\n",
      "Saving model MLP\n",
      "Time step 419\n",
      "Saving model MLP\n",
      "Time step 420\n",
      "Saving model MLP\n",
      "Time step 421\n",
      "Saving model MLP\n",
      "Time step 422\n",
      "Saving model MLP\n",
      "Time step 423\n",
      "Saving model MLP\n",
      "Time step 424\n",
      "Saving model MLP\n",
      "Time step 425\n",
      "Saving model MLP\n",
      "Time step 426\n",
      "Saving model MLP\n",
      "Time step 427\n",
      "Saving model MLP\n",
      "Time step 428\n",
      "Saving model MLP\n",
      "Time step 429\n",
      "Saving model MLP\n",
      "Time step 430\n",
      "Saving model MLP\n",
      "Time step 431\n",
      "Saving model MLP\n",
      "Time step 432\n",
      "Saving model MLP\n",
      "Time step 433\n",
      "Saving model MLP\n",
      "Time step 434\n",
      "Saving model MLP\n",
      "Time step 435\n",
      "Saving model MLP\n",
      "Time step 436\n",
      "Saving model MLP\n",
      "Time step 437\n",
      "Saving model MLP\n",
      "Time step 438\n",
      "Saving model MLP\n",
      "Time step 439\n",
      "Saving model MLP\n",
      "Time step 440\n",
      "Saving model MLP\n",
      "Time step 441\n",
      "Saving model MLP\n",
      "Time step 442\n",
      "Saving model MLP\n",
      "Time step 443\n",
      "Saving model MLP\n",
      "Time step 444\n",
      "Saving model MLP\n",
      "Time step 445\n",
      "Saving model MLP\n",
      "Time step 446\n",
      "Saving model MLP\n",
      "Time step 447\n",
      "Saving model MLP\n",
      "Time step 448\n",
      "Saving model MLP\n",
      "Time step 449\n",
      "Saving model MLP\n",
      "Time step 450\n",
      "Saving model MLP\n",
      "Time step 451\n",
      "Saving model MLP\n",
      "Time step 452\n",
      "Saving model MLP\n",
      "Time step 453\n",
      "Saving model MLP\n",
      "Time step 454\n",
      "Saving model MLP\n",
      "Time step 455\n",
      "Saving model MLP\n",
      "Time step 456\n",
      "Saving model MLP\n",
      "Time step 457\n",
      "Saving model MLP\n",
      "Time step 458\n",
      "Saving model MLP\n",
      "Time step 459\n",
      "Saving model MLP\n",
      "Time step 460\n",
      "Saving model MLP\n",
      "Time step 461\n",
      "Saving model MLP\n",
      "Time step 462\n",
      "Saving model MLP\n",
      "Time step 463\n",
      "Saving model MLP\n",
      "Time step 464\n",
      "Saving model MLP\n",
      "Time step 465\n",
      "Saving model MLP\n",
      "Time step 466\n",
      "Saving model MLP\n",
      "Time step 467\n",
      "Saving model MLP\n",
      "Time step 468\n",
      "Saving model MLP\n",
      "Time step 469\n",
      "Saving model MLP\n",
      "Time step 470\n",
      "Saving model MLP\n",
      "Time step 471\n",
      "Saving model MLP\n",
      "Time step 472\n",
      "Saving model MLP\n",
      "Time step 473\n",
      "Saving model MLP\n",
      "Time step 474\n",
      "Saving model MLP\n",
      "Time step 475\n",
      "Saving model MLP\n",
      "Time step 476\n",
      "Saving model MLP\n",
      "Time step 477\n",
      "Saving model MLP\n",
      "Time step 478\n",
      "Saving model MLP\n",
      "Time step 479\n",
      "Saving model MLP\n",
      "Time step 480\n",
      "Saving model MLP\n",
      "Time step 481\n",
      "Saving model MLP\n",
      "Time step 482\n",
      "Saving model MLP\n",
      "Time step 483\n",
      "Saving model MLP\n",
      "Time step 484\n",
      "Saving model MLP\n",
      "Time step 485\n",
      "Saving model MLP\n",
      "Time step 486\n",
      "Saving model MLP\n",
      "Time step 487\n",
      "Saving model MLP\n",
      "Time step 488\n",
      "Saving model MLP\n",
      "Time step 489\n",
      "Saving model MLP\n",
      "Time step 490\n",
      "Saving model MLP\n",
      "Time step 491\n",
      "Saving model MLP\n",
      "Time step 492\n",
      "Saving model MLP\n",
      "Time step 493\n",
      "Saving model MLP\n",
      "Time step 494\n",
      "Saving model MLP\n",
      "Time step 495\n",
      "Saving model MLP\n",
      "Time step 496\n",
      "Saving model MLP\n",
      "Time step 497\n",
      "Saving model MLP\n",
      "Time step 498\n",
      "Saving model MLP\n",
      "Time step 499\n",
      "Saving model MLP\n",
      "Time step 500\n",
      "Saving model MLP\n",
      "Time step 501\n",
      "Saving model MLP\n",
      "Time step 502\n",
      "Saving model MLP\n",
      "End time reached.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Pre-process: clean existing data in spinningDisk.\n",
    "    res_allrun_clean = subprocess.call(['bash', './Allclean'])\n",
    "    print(f'Allclean executed with return code: {res_allrun_clean}')\n",
    "    # Pre-process: create a mesh and decompose the solution domain of spinningDisk \n",
    "    # - Pre-processing does not interact with ML, so SmartSim models are not used.\n",
    "    res_allrun_pre = subprocess.call(['bash', './Allrun.pre'])\n",
    "    print(f'Allrun.pre executed with return code: {res_allrun_pre}')\n",
    "    \n",
    "    # Run the experiment\n",
    "    exp.start(of_model, block=False)\n",
    "\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = MLP(num_layers=3, layer_width=50, input_size=2, output_size=2, activation_fn=torch.nn.Sigmoid())\n",
    "    \n",
    "    # Make sure all datasets are avaialble in the smartredis database.\n",
    "    local_time_index = 1\n",
    "    while True:\n",
    "\n",
    "        print (f\"Time step {local_time_index}\")\n",
    "          \n",
    "        # Fetch datasets from SmartRedis\n",
    " \n",
    "        # - Poll until the points datasets are written by OpenFOAM\n",
    "        # print (f\"dataset_list_length {dataset_list_length}\") # Debug info\n",
    "        points_updated = client.poll_list_length(\"pointsDatasetList\", \n",
    "                                                 num_mpi_ranks, 10, 1000);\n",
    "        if (not points_updated):\n",
    "            raise ValueError(\"Points dataset list not updated.\")\n",
    "            \n",
    "        # - Poll until the displacements datasets are written by OpenFOAM\n",
    "        # print (f\"dataset_list_length {dataset_list_length}\") # Debug info\n",
    "        displacements_updated = client.poll_list_length(\"displacementsDatasetList\", \n",
    "                                                         num_mpi_ranks, 10, 1000);\n",
    "        if (not displacements_updated):\n",
    "            raise ValueError(\"Displacements dataset list not updated.\")\n",
    "            \n",
    "        # - Get the points and displacements datasets from SmartRedis\n",
    "        points_datasets = client.get_datasets_from_list(\"pointsDatasetList\")  \n",
    "        displacements_datasets = client.get_datasets_from_list(\"displacementsDatasetList\")\n",
    "        \n",
    "        # - Agglomerate all tensors from points and displacements datasets: \n",
    "        #   sort tensors by their names to ensure matching patches of same MPI ranks\n",
    "        points = []\n",
    "        points_names = []\n",
    "        displacements = []\n",
    "        displacements_names = []\n",
    "\n",
    "        # Agglomerate boudary points and displacements for training.\n",
    "        # TODO(TM): for mesh motion, send points_MPI_r, displacements_MPI_r and \n",
    "        #           train the MLP directly on the tensors, there is no need to \n",
    "        #           differentiate the BCs, as values are used for the training. \n",
    "        for points_dset, displs_dset in zip(points_datasets, displacements_datasets):\n",
    "            points_tensor_names = points_dset.get_tensor_names()\n",
    "            displs_tensor_names = displs_dset.get_tensor_names()\n",
    "            for points_name,displs_name in zip(points_tensor_names,displs_tensor_names):\n",
    "                patch_points = points_dset.get_tensor(points_name)\n",
    "                points.append(patch_points)\n",
    "                points_names.append(points_name)\n",
    "\n",
    "                patch_displs = displs_dset.get_tensor(displs_name)\n",
    "                displacements.append(patch_displs)\n",
    "                displacements_names.append(displs_name)\n",
    "                \n",
    "        points, points_names = sort_tensors_by_names(points, points_names)\n",
    "        displacements, displacements_names = sort_tensors_by_names(displacements, displacements_names)\n",
    "        \n",
    "        # - Reshape points and displacements into [N_POINTS,SPATIAL_DIMENSION] tensors\n",
    "        #   This basically agglomerates data from OpenFOAM boundary patches into a list\n",
    "        #   of boundary points (unstructured) and a list of respective point displacements. \n",
    "        points = torch.from_numpy(np.vstack(points))\n",
    "        displacements = torch.from_numpy(np.vstack(displacements))\n",
    "        \n",
    "        # TODO(TM): hardcoded x,y coordinates, make the OF client store polymesh::solutionD\n",
    "        #           and use solutionD non-zero values for sampling vector coordinates. \n",
    "        points = points[:, :2]\n",
    "        displacements = displacements[:, :2]\n",
    "\n",
    "        # Split training and validation data\n",
    "        points_train, points_val, displ_train, displ_val = train_test_split(points, displacements, \n",
    "                                                                            test_size=0.2, random_state=42)\n",
    "\n",
    "        # PYTORCH Training Loop\n",
    "        if local_time_index == 1: # provide sufficient epochs in the first time step\n",
    "            epochs = 10000\n",
    "            learning_rate = 1e-04\n",
    "        else: # Use the benefit of transfer-learning\n",
    "            epochs = 1000 \n",
    "            learning_rate = 1e-05\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        loss_func = nn.MSELoss()\n",
    "      \n",
    "        mean_mag_displ = torch.mean(torch.norm(displ_train, dim=1))\n",
    "        validation_rmse = []\n",
    "        model.train()\n",
    "        for epoch in range(epochs):    \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass on the training data\n",
    "            displ_pred = model(points_train)\n",
    "\n",
    "            # Compute loss on the training data\n",
    "            loss_train = loss_func(displ_pred, displ_train)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Forward pass on the validation data, with torch.no_grad() for efficiency\n",
    "            with torch.no_grad():\n",
    "                displ_pred_val = model(points_val)\n",
    "                mse_loss_val = loss_func(displ_pred_val, displ_val)\n",
    "                rmse_loss_val = torch.sqrt(mse_loss_val)\n",
    "                print (f\"RMSE {rmse_loss_val}\")\n",
    "                validation_rmse.append(rmse_loss_val)\n",
    "\n",
    "        # Uncomment to visualize validation RMSE\n",
    "        # plt.loglog()\n",
    "        # plt.title(\"Validation loss RMSE\")\n",
    "        # plt.xlabel(\"Epochs\")\n",
    "        # plt.plot(validation_rmse)\n",
    "        # plt.show()\n",
    "\n",
    "        # Store the model into SmartRedis\n",
    "        model.eval() # TEST\n",
    "        # Prepare a sample input\n",
    "        example_forward_input = torch.rand(2)\n",
    "        # Convert the PyTorch model to TorchScript\n",
    "        model_script = torch.jit.trace(model, example_forward_input)\n",
    "        # Save the TorchScript model to a buffer\n",
    "        model_buffer = io.BytesIO()\n",
    "        torch.jit.save(model_script, model_buffer)\n",
    "        # Set the model in the SmartRedis database\n",
    "        print(\"Saving model MLP\")\n",
    "        client.set_model(\"MLP\", model_buffer.getvalue(), \"TORCH\", \"CPU\")\n",
    "\n",
    "        # Update the model in smartredis\n",
    "        client.put_tensor(\"model_updated\", np.array([0.]))\n",
    "\n",
    "        # Delete dataset lists for the next time step\n",
    "        client.delete_list(\"pointsDatasetList\")\n",
    "        client.delete_list(\"displacementsDatasetList\")\n",
    "\n",
    "        # Update time index\n",
    "        local_time_index = local_time_index + 1\n",
    "\n",
    "        if client.poll_key(\"end_time_index\", 10, 10):\n",
    "            print (\"End time reached.\")\n",
    "            break\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Caught an exception: \", str(e))\n",
    "    \n",
    "finally:\n",
    "    exp.stop(db)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
